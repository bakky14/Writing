{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5496dae6-0c2d-46c3-aee5-af396d044b7b",
   "metadata": {},
   "source": [
    "# 8.2 Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5cfa2f-30ae-4412-8291-517d8450bf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a35476-651a-40f6-869c-e8151aa98afa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.3 SFT: Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc9d7a-530c-4ebf-bb19-3729bb240181",
   "metadata": {},
   "source": [
    "## quick start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b12a2fd8-d077-477d-8a90-5cf862d5238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw/.pyenv/versions/3.10.13/envs/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    ")\n",
    "# 학습시에 주석 해제\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81d091-1228-4c3a-b54b-fb72ee7d16ad",
   "metadata": {},
   "source": [
    "## setup chat format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9044113-476e-46d5-908c-fafbe45ba17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: None\n",
      "after: {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import setup_chat_format\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "print(\"before:\", tokenizer.chat_template)\n",
    "\n",
    "# Set up the chat format with default 'chatml' format\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "print(\"after:\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318f77a-aae9-4f95-8f5a-1ba9b764f7ae",
   "metadata": {},
   "source": [
    "## formatting func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd120275-41b6-4a03-a11a-999b03a4fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['question'])):\n",
    "        text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423f90d-e10a-4d3d-af75-1b6e233ecf81",
   "metadata": {},
   "source": [
    "## packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8975e9d9-7f94-4913-8992-29b13271fec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47733094048044c5bb4d79e4854c4510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4890b-2c56-44aa-bc4a-fbdc934491bb",
   "metadata": {},
   "source": [
    "## model_init_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5524b214-0dfc-4813-85d8-dd9752918da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    model_init_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"attn_implementation\": \"flash_attention_2\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ed1d7-d403-443f-9cba-f70cd22e586c",
   "metadata": {},
   "source": [
    "## peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b95c9ee-80f8-4d63-a25a-0ea470e4d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw/.pyenv/versions/3.10.13/envs/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86e6db38-3ef1-4582-8ace-554e93842d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw/.pyenv/versions/3.10.13/envs/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    model_init_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"load_in_4bit\": True,\n",
    "        \"attn_implementation\": \"flash_attention_2\"\n",
    "    },\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77337f9-36d8-4992-83e5-9033466ac11d",
   "metadata": {},
   "source": [
    "## model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d214967f-c72f-4acd-af1d-10c86a5a955d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_name_or_path='facebook/opt-350m', model_revision='main', torch_dtype=None, trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import ModelConfig, SFTTrainer, get_kbit_device_map, get_peft_config, get_quantization_config\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"facebook/opt-350m\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    load_in_4bit=True,\n",
    "    use_peft=True,\n",
    ")\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a68533b-4302-4c19-a1a1-53f6f32433a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"float32\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": false,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config = get_quantization_config(model_config)\n",
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc9b17da-2c01-451e-b010-2322ea621a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_kbit_device_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05dedd54-bf54-4f29-8dc5-11b6fdcce821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = get_peft_config(model_config)\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa248ced-71b6-4161-ad11-b8faa315342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = (\n",
    "    model_config.torch_dtype\n",
    "    if model_config.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_config.torch_dtype)\n",
    ")\n",
    "model_kwargs = dict(\n",
    "    revision=model_config.model_revision,\n",
    "    trust_remote_code=model_config.trust_remote_code,\n",
    "    attn_implementation=model_config.attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    model_init_kwargs=model_kwargs,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a065aa5-29bc-460d-a4b3-a7b40554e926",
   "metadata": {},
   "source": [
    "## neftune_noise_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "850a00bf-d498-4aff-8875-920855f26f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw/.pyenv/versions/3.10.13/envs/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7d935b657a41df94ef7a3e517be0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    neftune_noise_alpha=5,\n",
    ")\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0099b-f7b3-4adf-acdd-2965272152e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.4 PPO: Proximal Policy Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7d1cfb-7f23-4c2a-9076-cf418475e7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Explain the moon landing to a 6 year old in a few sentences.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.remove_columns([\"meta\", \"completion\"])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ba0e01-19c2-4d3f-b064-b8c83b8ea2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5064ad68c2634a13b4812e2e973ebf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500cc0d6a77b4ad98ada9292e5793ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b633e72373d24b70adf0771c52bb4c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dff41913b4f43bfb2b80a7518490279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796c24ca666d46febf220ca96ae5c493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0c6ce1d91e410c87aaa9123097a385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949f52a75df84baea5d799181e660966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133cf46696d64e578944cda00bdb348d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",\n",
    "    learning_rate=1.41e-5,\n",
    ")\n",
    "model = (\n",
    "    AutoModelForCausalLMWithValueHead\n",
    "    .from_pretrained(config.model_name)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "# TDDO: 이거 eos 붙여서 학습해야하나?\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9ecbf0-a6a9-4c84-aad5-45fb43577d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8be8aad2526439681bc032be64ae6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4708632c30f44a598738ce3aa46f37c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d943d9a509c4037b86dc6e5b7f3f0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16c76bbdde44176b92f131dd012971a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00a1781f2c6403c99d55e1685ace586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bc2716b98b483289a192dfe0624965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "reward_model = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=\"lvwerra/distilbert-imdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a32441-343d-459b-aece-3d86226ff132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b50e3906784aa08f998aa03b7df5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(sample):\n",
    "    # eos 붙여서?\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd33d74-77e8-43b2-86b6-bbcd28775ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3bbd0c-406d-4e16-9cfe-b32218a7dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b488b-4a31-4ea6-b4a9-266ba04f3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader): \n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "    \n",
    "        # Rollout: 학습할 모델로 문장 생성\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            query_tensors, \n",
    "            **generation_kwargs\n",
    "        )\n",
    "        batch[\"response\"] = [\n",
    "            tokenizer.decode(r.squeeze()) \n",
    "            for r in response_tensors\n",
    "        ]\n",
    "    \n",
    "        # Evaluate: Reward 모델로 점수 부여\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = reward_model(texts)\n",
    "        rewards = [\n",
    "            torch.tensor(output[1][\"score\"]) \n",
    "            for output in pipe_outputs\n",
    "        ]\n",
    "    \n",
    "        # Optimization: ppo 학습 진행\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "# 모델 저장\n",
    "ppo_trainer.save_pretrained(\"my_ppo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac04b52-2d2c-49d7-83ad-909964cc3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer.tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f05fe-ffa6-4c8f-bf38-787ad4643659",
   "metadata": {},
   "source": [
    "# 8.5 Best of N Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a53734-522b-4f73-a684-de930a0d5591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, GenerationConfig\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "from trl.extras import BestOfNSampler\n",
    "\n",
    "ref_model_name = \"gpt2\"\n",
    "reward_model_name = \"gpt2\"\n",
    "device = torch.device(\"cuda\")\n",
    "ref_model = (\n",
    "    AutoModelForCausalLMWithValueHead\n",
    "    .from_pretrained(ref_model_name)\n",
    "    .to(device)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ref_model_name)\n",
    "\n",
    "reward_pipe = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=reward_model_name, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "def queries_to_scores(list_of_strings):\n",
    "    return [output[\"score\"] for output in reward_pipe(list_of_strings)]\n",
    "\n",
    "best_of_n = BestOfNSampler(\n",
    "    ref_model, \n",
    "    tokenizer, \n",
    "    queries_to_scores, \n",
    "    length_sampler=LengthSampler(10, 128), \n",
    "    sample_size=5,\n",
    "    n_candidates=2,\n",
    "    generation_config=GenerationConfig(\n",
    "        min_length= -1, \n",
    "        top_k=0.0, \n",
    "        top_p= 1.0, \n",
    "        do_sample= True, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b73ae3c-328e-4259-a6b0-f1d8046f07c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is love? Love? Where can I find it?\n",
      "\n",
      "Love love, what are you love loving?\n",
      "\n",
      "Love love, what is the Holiness to you?\n",
      "\n",
      "Love feeling love, what is the Glory of God in you?\n",
      "\n",
      "Love love, what does it mean to love?\n",
      "\n",
      "Love. Love who has\n",
      "================================================== \n",
      "\n",
      "what is love? How has love been changed? How things would change under God? Do God's desires continue with the child? Does love diminish with society's powers? Should people control themselves? At times these questions have seemed theological, yet of all the demanding questions that Jesus confronts, surely we get the most devastated reading.\n",
      "\n",
      "John Piper's\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = best_of_n.generate(\n",
    "    tokenizer(\"what is love?\", return_tensors=\"pt\").input_ids[0], \n",
    "    device=device\n",
    ")\n",
    "\n",
    "for r in result[0]:\n",
    "    print(r)\n",
    "    print(\"=\" * 50, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df7066-88e8-4b18-a4f1-aeb81affbab2",
   "metadata": {},
   "source": [
    "# 8.6 DPO: Directi Preference Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b9de5-2ad5-49a5-bf76-44fc4e51250c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d7c0149-1111-4225-aca4-a3e47efcf6c5",
   "metadata": {},
   "source": [
    "# 8.7 KTO: Kahneman-Tversky Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bfd19-05b1-4194-874d-0adfe4bc0bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
