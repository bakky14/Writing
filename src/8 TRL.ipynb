{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5496dae6-0c2d-46c3-aee5-af396d044b7b",
   "metadata": {
    "id": "5496dae6-0c2d-46c3-aee5-af396d044b7b",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.3 Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99536f-4868-4b5e-9f5c-fd3f8d5a9c2f",
   "metadata": {
    "id": "7b99536f-4868-4b5e-9f5c-fd3f8d5a9c2f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/opt-350m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7c30d-8092-4fdf-9eb1-15a37d344bda",
   "metadata": {
    "id": "0bc7c30d-8092-4fdf-9eb1-15a37d344bda",
    "outputId": "787dab53-7d98-4db2-9bfd-4936b1ae842c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected'],\n",
       "    num_rows: 160800\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebb1a8-aa5f-4c38-9d47-2fe468d7858b",
   "metadata": {
    "id": "25ebb1a8-aa5f-4c38-9d47-2fe468d7858b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    result = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "    for chosen, rejected in zip(batch[\"chosen\"], batch[\"rejected\"]):\n",
    "        tokenized_chosen = tokenizer(chosen)\n",
    "        tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "        result[\"input_ids_chosen\"].append(\n",
    "            tokenized_chosen[\"input_ids\"]\n",
    "        )\n",
    "        result[\"attention_mask_chosen\"].append(\n",
    "            tokenized_chosen[\"attention_mask\"]\n",
    "        )\n",
    "        result[\"input_ids_rejected\"].append(\n",
    "            tokenized_rejected[\"input_ids\"]\n",
    "        )\n",
    "        result[\"attention_mask_rejected\"].append(\n",
    "            tokenized_rejected[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    num_proc=2,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "max_length = 512\n",
    "dataset = dataset.filter(\n",
    "    lambda x: (\n",
    "        len(x[\"input_ids_chosen\"]) <= max_length\n",
    "        and len(x[\"input_ids_rejected\"]) <= max_length\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5cfa2f-30ae-4412-8291-517d8450bf35",
   "metadata": {
    "id": "8b5cfa2f-30ae-4412-8291-517d8450bf35"
   },
   "outputs": [],
   "source": [
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "config = RewardConfig(\n",
    "    logging_dir=\"logs\",\n",
    "    output_dir=\"ckpt\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=config,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a35476-651a-40f6-869c-e8151aa98afa",
   "metadata": {
    "id": "15a35476-651a-40f6-869c-e8151aa98afa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.4 SFT: Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc9d7a-530c-4ebf-bb19-3729bb240181",
   "metadata": {
    "id": "f3bc9d7a-530c-4ebf-bb19-3729bb240181",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8.3.1 기본 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a2fd8-d077-477d-8a90-5cf862d5238a",
   "metadata": {
    "id": "b12a2fd8-d077-477d-8a90-5cf862d5238a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    output_dir=\"./ckpt\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config,\n",
    ")\n",
    "# trainer.train()  # 실제 학습할때만 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BWNDXuV9mEH0",
   "metadata": {
    "id": "BWNDXuV9mEH0"
   },
   "source": [
    "DatacollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d5b3e6-2289-4d05-8f41-079bc4bfc219",
   "metadata": {
    "id": "88d5b3e6-2289-4d05-8f41-079bc4bfc219",
    "outputId": "00b09925-3804-4225-cbfb-de6a043cc2db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100,   42,   16, 1263,    4,   16,   24,  173,  116,    2])\n",
      "only response:  this is response. is it work?</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=\" [/INST]\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "prompt_example = \"<s>[INST] this is input prompt [/INST] this is response. is it work?</s>\"\n",
    "example = collator([tokenizer(prompt_example)])\n",
    "\n",
    "label = example.labels[0]\n",
    "print(label)\n",
    "print(\"only response:\", tokenizer.decode(label[label > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e068b0c-9f29-4106-a86d-8d31af46d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 10975), ('INST', 39236), (']', 742), ('Ġthis', 42), ('Ġis', 16), ('Ġinput', 8135), ('Ġprompt', 14302), ('Ġ[/', 48651), ('INST', 39236), (']', 742), ('Ġthis', 42), ('Ġis', 16), ('Ġresponse', 1263), ('.', 4), ('Ġis', 16), ('Ġit', 24), ('Ġwork', 173), ('?', 116)]\n",
      "[('[/', 48505), ('INST', 39236), (']', 742)]\n"
     ]
    }
   ],
   "source": [
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))\n",
    "\n",
    "prompt = \"[INST] this is input prompt [/INST] this is response. is it work?\"\n",
    "print_tokens_with_ids(prompt)\n",
    "\n",
    "response_template = \"[/INST]\"\n",
    "print_tokens_with_ids(response_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81d091-1228-4c3a-b54b-fb72ee7d16ad",
   "metadata": {
    "id": "2f81d091-1228-4c3a-b54b-fb72ee7d16ad",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "setup chat format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9044113-476e-46d5-908c-fafbe45ba17d",
   "metadata": {
    "id": "a9044113-476e-46d5-908c-fafbe45ba17d",
    "outputId": "9e02c07f-4641-43bf-dd0f-f5e3fb46c6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: None\n",
      "after: {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import setup_chat_format\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "print(\"before:\", tokenizer.chat_template)\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "print(\"after:\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318f77a-aae9-4f95-8f5a-1ba9b764f7ae",
   "metadata": {
    "id": "e318f77a-aae9-4f95-8f5a-1ba9b764f7ae",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "formatting func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd120275-41b6-4a03-a11a-999b03a4fcb2",
   "metadata": {
    "id": "bd120275-41b6-4a03-a11a-999b03a4fcb2"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "example = [\n",
    "    {\"question\": \"질문 1\", \"answer\": \"답변 1\"},\n",
    "    {\"question\": \"질문 2\", \"answer\": \"답변 2\"},\n",
    "    {\"question\": \"질문 3\", \"answer\": \"답변 3\"},\n",
    "]\n",
    "test_dataset = Dataset.from_list(example)\n",
    "\n",
    "def formatting_prompts_func(sample):\n",
    "    output_texts = []\n",
    "    for i in range(len(sample[\"question\"])):\n",
    "        text = (\n",
    "            f\"### Question: {sample['question'][i]}\\n \"\n",
    "            f\"### Answer: {sample['answer'][i]}\"\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    output_dir=\"./ckpt\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# trainer.train()  # 실제 학습할때만 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423f90d-e10a-4d3d-af75-1b6e233ecf81",
   "metadata": {
    "id": "1423f90d-e10a-4d3d-af75-1b6e233ecf81",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975e9d9-7f94-4913-8992-29b13271fec7",
   "metadata": {
    "id": "8975e9d9-7f94-4913-8992-29b13271fec7"
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    packing=True, \n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    output_dir=\"./ckpt\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config\n",
    ")\n",
    "\n",
    "# trainer.train()  # 실제 학습할때만 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4890b-2c56-44aa-bc4a-fbdc934491bb",
   "metadata": {
    "id": "28f4890b-2c56-44aa-bc4a-fbdc934491bb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## model_init_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524b214-0dfc-4813-85d8-dd9752918da7",
   "metadata": {
    "id": "5524b214-0dfc-4813-85d8-dd9752918da7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    model_init_kwargs={\n",
    "        \"torch_dtype\": \"bfloat16\",\n",
    "    },\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    output_dir=\"./ckpt\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "# trainer.train()  # 실제 학습할때만 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ed1d7-d403-443f-9cba-f70cd22e586c",
   "metadata": {
    "id": "6a3ed1d7-d403-443f-9cba-f70cd22e586c",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95c9ee-80f8-4d63-a25a-0ea470e4d92c",
   "metadata": {
    "id": "6b95c9ee-80f8-4d63-a25a-0ea470e4d92c"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    args=SFTConfig(\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"text\",\n",
    "        output_dir=\"./ckpt\",\n",
    "    ),\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "# trainer.train()  # 실제 학습할때만 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6db38-3ef1-4582-8ace-554e93842d2b",
   "metadata": {
    "id": "86e6db38-3ef1-4582-8ace-554e93842d2b"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    args=SFTConfig(\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"text\",\n",
    "        output_dir=\"./ckpt\",\n",
    "    ),\n",
    "    model_init_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"load_in_4bit\": True,\n",
    "    },\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# trainer.train()  # 실제 학습할때만 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77337f9-36d8-4992-83e5-9033466ac11d",
   "metadata": {
    "id": "d77337f9-36d8-4992-83e5-9033466ac11d",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d214967f-c72f-4acd-af1d-10c86a5a955d",
   "metadata": {
    "id": "d214967f-c72f-4acd-af1d-10c86a5a955d",
    "outputId": "15fbe37a-ca89-4c61-82ca-4145fe23fcdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_name_or_path='facebook/opt-350m', model_revision='main', torch_dtype=None, trust_remote_code=False, attn_implementation=None, use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import (\n",
    "    ModelConfig,\n",
    "    SFTTrainer,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"facebook/opt-350m\",\n",
    "    load_in_4bit=True,\n",
    "    use_peft=True,\n",
    ")\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a68533b-4302-4c19-a1a1-53f6f32433a2",
   "metadata": {
    "id": "9a68533b-4302-4c19-a1a1-53f6f32433a2",
    "outputId": "c6c8a5d2-2c4b-417d-8104-f4b6f06c9561"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"float32\",\n",
       "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": false,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config = get_quantization_config(model_config)\n",
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9b17da-2c01-451e-b010-2322ea621a43",
   "metadata": {
    "id": "dc9b17da-2c01-451e-b010-2322ea621a43",
    "outputId": "3c2aa347-6cba-4bb4-d7e7-814e1fc773c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_kbit_device_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05dedd54-bf54-4f29-8dc5-11b6fdcce821",
   "metadata": {
    "id": "05dedd54-bf54-4f29-8dc5-11b6fdcce821",
    "outputId": "504b3118-75f2-4cdd-ac02-202d8820ef65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = get_peft_config(model_config)\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa248ced-71b6-4161-ad11-b8faa315342c",
   "metadata": {
    "id": "aa248ced-71b6-4161-ad11-b8faa315342c"
   },
   "outputs": [],
   "source": [
    "torch_dtype = (\n",
    "    model_config.torch_dtype\n",
    "    if model_config.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_config.torch_dtype)\n",
    ")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    revision=model_config.model_revision,\n",
    "    trust_remote_code=model_config.trust_remote_code,\n",
    "    attn_implementation=model_config.attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=(\n",
    "        get_kbit_device_map() \n",
    "        if quantization_config is not None \n",
    "        else None\n",
    "    ),\n",
    "    quantization_config=quantization_config.to_dict(),\n",
    ")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    output_dir=\"./ckpt\",\n",
    "    model_init_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# trainer.train()  # 실제 학습할때만 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a065aa5-29bc-460d-a4b3-a7b40554e926",
   "metadata": {
    "id": "9a065aa5-29bc-460d-a4b3-a7b40554e926",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## neftune_noise_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a00bf-d498-4aff-8875-920855f26f32",
   "metadata": {
    "id": "850a00bf-d498-4aff-8875-920855f26f32"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=dataset,\n",
    "    args=SFTConfig(\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"text\",\n",
    "        output_dir=\"./ckpt\",\n",
    "        neftune_noise_alpha=5,\n",
    "    ),\n",
    ")\n",
    "# trainer.train()  # 실제 학습할때만 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0099b-f7b3-4adf-acdd-2965272152e7",
   "metadata": {
    "id": "4ca0099b-f7b3-4adf-acdd-2965272152e7"
   },
   "source": [
    "# 8.5 PPO: Proximal Policy Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ba0e01-19c2-4d3f-b064-b8c83b8ea2f9",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "5064ad68c2634a13b4812e2e973ebf26",
      "500cc0d6a77b4ad98ada9292e5793ebe",
      "b633e72373d24b70adf0771c52bb4c91",
      "3dff41913b4f43bfb2b80a7518490279",
      "796c24ca666d46febf220ca96ae5c493",
      "fe0c6ce1d91e410c87aaa9123097a385",
      "949f52a75df84baea5d799181e660966",
      "133cf46696d64e578944cda00bdb348d"
     ]
    },
    "id": "36ba0e01-19c2-4d3f-b064-b8c83b8ea2f9",
    "outputId": "114793bc-a392-41c3-bdc8-5f306da40e61"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",\n",
    "    learning_rate=1.41e-5,\n",
    "    mini_batch_size=1,\n",
    "    batch_size=1,\n",
    ")\n",
    "model = (\n",
    "    AutoModelForCausalLMWithValueHead\n",
    "    .from_pretrained(config.model_name)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9ecbf0-a6a9-4c84-aad5-45fb43577d94",
   "metadata": {
    "id": "8e9ecbf0-a6a9-4c84-aad5-45fb43577d94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "reward_model = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"lvwerra/distilbert-imdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7d1cfb-7f23-4c2a-9076-cf418475e7ae",
   "metadata": {
    "id": "af7d1cfb-7f23-4c2a-9076-cf418475e7ae",
    "outputId": "8fb86660-09ac-4154-ba46-b7c037b6e5ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Explain the moon landing to a 6 year old in a few sentences.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"HuggingFaceH4/cherry_picked_prompts\",\n",
    "    split=\"train\",\n",
    ")\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.remove_columns([\"meta\", \"completion\"])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a32441-343d-459b-aece-3d86226ff132",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d9b50e3906784aa08f998aa03b7df5b8"
     ]
    },
    "id": "d7a32441-343d-459b-aece-3d86226ff132",
    "outputId": "775ec2ff-81b3-46e1-fcf5-57977c73825e"
   },
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=False)\n",
    "\n",
    "dataset.set_format(type=\"torch\")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d158720c-78d9-4932-a57a-d0df2e39ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_length\": 256,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "dataset = dataset.filter(\n",
    "    lambda x: len(x[\"input_ids\"]) <= generation_kwargs['max_length']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd33d74-77e8-43b2-86b6-bbcd28775ac7",
   "metadata": {
    "id": "2bd33d74-77e8-43b2-86b6-bbcd28775ac7"
   },
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b488b-4a31-4ea6-b4a9-266ba04f3593",
   "metadata": {
    "id": "840b488b-4a31-4ea6-b4a9-266ba04f3593"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102b5e9f87fc46d0812902be83174906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87bcc0afb8e432896dde132331d78e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1300: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
      "  std_scores = data[\"scores\"].std()\n",
      "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1327: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
      "  stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n",
      "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1330: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
      "  stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n",
      "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1399: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
      "  logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "        # Rollout: 학습할 모델로 문장 생성\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        batch[\"response\"] = [\n",
    "            tokenizer.decode(r.squeeze())\n",
    "            for r in response_tensors\n",
    "        ]\n",
    "\n",
    "        # Evaluate: Reward 모델로 점수 부여\n",
    "        # return_full_text 옵션 입력이 불가능하므로, response == query + gen_text\n",
    "        pipe_outputs = reward_model(batch[\"response\"])\n",
    "        rewards = [\n",
    "            torch.tensor(output[\"score\"])\n",
    "            for output in pipe_outputs\n",
    "        ]\n",
    "\n",
    "        # Optimization: ppo 학습 진행\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "# 모델 저장\n",
    "ppo_trainer.save_pretrained(\"/content/drive/MyDrive/Books/outputs/my_ppo_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f05fe-ffa6-4c8f-bf38-787ad4643659",
   "metadata": {
    "id": "cd4f05fe-ffa6-4c8f-bf38-787ad4643659",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.6 Best of N Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a53734-522b-4f73-a684-de930a0d5591",
   "metadata": {
    "id": "87a53734-522b-4f73-a684-de930a0d5591",
    "outputId": "de38ba6a-8300-4c05-9f38-f164bdd5caff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, GenerationConfig\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "from trl.extras import BestOfNSampler\n",
    "\n",
    "ref_model_name = \"gpt2\"\n",
    "reward_model_name = \"gpt2\"\n",
    "device = torch.device(\"cuda\")\n",
    "ref_model = (\n",
    "    AutoModelForCausalLMWithValueHead\n",
    "    .from_pretrained(ref_model_name)\n",
    "    .to(device)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ref_model_name)\n",
    "\n",
    "reward_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=reward_model_name,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def queries_to_scores(list_of_strings):\n",
    "    return [output[\"score\"] for output in reward_pipe(list_of_strings)]\n",
    "\n",
    "best_of_n = BestOfNSampler(\n",
    "    ref_model,\n",
    "    tokenizer,\n",
    "    queries_to_scores,\n",
    "    length_sampler=LengthSampler(10, 128),\n",
    "    sample_size=5,\n",
    "    n_candidates=2,\n",
    "    generation_config=GenerationConfig(\n",
    "        min_length= -1,\n",
    "        top_k=0.0,\n",
    "        top_p= 1.0,\n",
    "        do_sample= True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73ae3c-328e-4259-a6b0-f1d8046f07c0",
   "metadata": {
    "id": "5b73ae3c-328e-4259-a6b0-f1d8046f07c0",
    "outputId": "0d60abae-1329-4c98-c526-4079db459b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is love? Love? Where can I find it?\n",
      "\n",
      "Love love, what are you love loving?\n",
      "\n",
      "Love love, what is the Holiness to you?\n",
      "\n",
      "Love feeling love, what is the Glory of God in you?\n",
      "\n",
      "Love love, what does it mean to love?\n",
      "\n",
      "Love. Love who has\n",
      "================================================== \n",
      "\n",
      "what is love? How has love been changed? How things would change under God? Do God's desires continue with the child? Does love diminish with society's powers? Should people control themselves? At times these questions have seemed theological, yet of all the demanding questions that Jesus confronts, surely we get the most devastated reading.\n",
      "\n",
      "John Piper's\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = best_of_n.generate(\n",
    "    tokenizer(\"what is love?\", return_tensors=\"pt\").input_ids[0],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "for r in result[0]:\n",
    "    print(r)\n",
    "    print(\"=\" * 50, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df7066-88e8-4b18-a4f1-aeb81affbab2",
   "metadata": {
    "id": "30df7066-88e8-4b18-a4f1-aeb81affbab2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.7 DPO: Directi Preference Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafbb629-0563-47ab-ae37-ebf6bf150285",
   "metadata": {
    "id": "bafbb629-0563-47ab-ae37-ebf6bf150285"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "ref_model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137b9de5-2ad5-49a5-bf76-44fc4e51250c",
   "metadata": {
    "id": "137b9de5-2ad5-49a5-bf76-44fc4e51250c",
    "outputId": "b0e90270-2547-4cf1-a91a-b4e1b0eb5e39"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37f430604c540e09a85eb3525b312b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84968c39f7cc461d9dc84edc27748aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 160800\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"trl-internal-testing/hh-rlhf-trl-style\",\n",
    "    split=\"train\",\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3135c81-ec72-4291-a642-643ee4c3b3e9",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b341a81728f946d18f152675e369db0b"
     ]
    },
    "id": "e3135c81-ec72-4291-a642-643ee4c3b3e9",
    "outputId": "60170dd5-e00d-4bec-bc66-3a5164a8ea6c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecd9cd841b44ac38e90f59914435b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = (\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{{message['role'] + ': ' + message['content'] + '\\n\\n'}}\"\n",
    "        \"{% endfor %}{{ eos_token }}\"\n",
    "    )\n",
    "\n",
    "def process(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(\n",
    "        row[\"chosen\"], tokenize=False\n",
    "    )\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(\n",
    "        row[\"rejected\"], tokenize=False\n",
    "    )\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    process,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40d91c-11ac-42e7-b6d5-db6d30d3f47f",
   "metadata": {
    "id": "ce40d91c-11ac-42e7-b6d5-db6d30d3f47f"
   },
   "outputs": [],
   "source": [
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "args = DPOConfig(\n",
    "    beta=0.1,\n",
    "    max_length=512,\n",
    "    max_prompt_length=512,\n",
    "    dataset_num_proc=2,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"./ckpt\",\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model, \n",
    "    ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c0149-1111-4225-aca4-a3e47efcf6c5",
   "metadata": {
    "id": "8d7c0149-1111-4225-aca4-a3e47efcf6c5",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.8 KTO: Kahneman-Tversky Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c203c2-8312-4d0c-bfdd-e33a9a433db5",
   "metadata": {
    "id": "92c203c2-8312-4d0c-bfdd-e33a9a433db5",
    "outputId": "39c7a047-b578-4744-fe19-205940e516b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"trl-lib/qwen1.5-1.8b-sft\"\n",
    "ref_model_name = \"trl-lib/qwen1.5-1.8b-sft\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bfd19-05b1-4194-874d-0adfe4bc0bd6",
   "metadata": {
    "id": "667bfd19-05b1-4194-874d-0adfe4bc0bd6",
    "outputId": "8d912193-6661-4a99-da33-ff8cf61d79d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'label'],\n",
       "    num_rows: 13500\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trl-lib/kto-mix-14k\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5124d-3fee-48c0-bc8c-7d0c6ae1ca1b",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "8e7e2da56d8a4c088312c649b7db7fbb"
     ]
    },
    "id": "dec5124d-3fee-48c0-bc8c-7d0c6ae1ca1b",
    "outputId": "0cf2875b-3ea7-4f84-cd4a-6b39c2ea140d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7e2da56d8a4c088312c649b7db7fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/13500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>assistant\n",
      "No, the answer provided does not directly answer the question regarding the age of Julio Cesar Chavez when he fought Oscar De La Hoya. The provided information describes some general records held by Julio Cesar Chavez throughout his career. To answer your original question, let me provide the relevant information:\n",
      "\n",
      "Julio Cesar Chavez fought Oscar De La Hoya on June 7, 1996, in a match called \"Ultimate Glory.\" Chavez was born on July 12, 1962. To calculate his age at the time of the fight, we need to find the difference between the fight date and his birthdate.\n",
      "\n",
      "From July 12, 1962, to June 7, 1996, there are:\n",
      "- 33 years (from 1962 to 1995)\n",
      "- An additional year from his birthday in 1995 (July 12, 1995) to the fight date in 1996 (June 7, 1996), which is approximately 10 months and 26 days.\n",
      "\n",
      "Therefore, Julio Cesar Chavez was about 33 years and 10 months old when he fought Oscar De La Hoya.<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "def process(row):\n",
    "    row[\"prompt\"] = tokenizer.apply_chat_template(\n",
    "        row[\"prompt\"], tokenize=False\n",
    "    )\n",
    "    row[\"completion\"] = tokenizer.apply_chat_template(\n",
    "        row[\"completion\"], tokenize=False\n",
    "    )\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    process,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "print(dataset[1]['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db4d43-f542-48a8-811a-864cdd812bc9",
   "metadata": {
    "id": "f9db4d43-f542-48a8-811a-864cdd812bc9"
   },
   "outputs": [],
   "source": [
    "from trl import KTOTrainer, KTOConfig\n",
    "\n",
    "args = KTOConfig(\n",
    "    logging_dir=\"logs\",\n",
    "    output_dir=\"ckpt\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=100,\n",
    "\n",
    "    max_length=512,\n",
    "    max_prompt_length=512,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_num_proc=2,\n",
    "\n",
    "    beta=0.1,\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=1.0,\n",
    ")\n",
    "\n",
    "trainer = KTOTrainer(\n",
    "    model,\n",
    "    ref_model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15acec3a-8f45-4971-8fa1-23922df7d55a",
   "metadata": {
    "id": "15acec3a-8f45-4971-8fa1-23922df7d55a",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.9 CPO: Contrastive Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccba57-34fb-4c38-9391-ce039faad2d1",
   "metadata": {
    "id": "3bccba57-34fb-4c38-9391-ce039faad2d1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b9d0a-f47e-4f0a-a927-6ac63b9a667c",
   "metadata": {
    "id": "778b9d0a-f47e-4f0a-a927-6ac63b9a667c",
    "outputId": "bf9af8d4-9bb0-4112-effc-debaf5b452b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 160800\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"trl-internal-testing/hh-rlhf-trl-style\",\n",
    "    split=\"train\",\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45127d14-aece-4131-8a5c-9a032ecc4e0a",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4bffb2fabd194b20b26395fd156060e2"
     ]
    },
    "id": "45127d14-aece-4131-8a5c-9a032ecc4e0a",
    "outputId": "195e9900-fa83-4e5d-e789-e947f1ecdc36"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bffb2fabd194b20b26395fd156060e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = (\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{{message['role'] + ': ' + message['content'] + '\\n\\n'}}\"\n",
    "        \"{% endfor %}{{ eos_token }}\"\n",
    "    )\n",
    "\n",
    "def process(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(\n",
    "        row[\"chosen\"], tokenize=False\n",
    "    )\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(\n",
    "        row[\"rejected\"], tokenize=False\n",
    "    )\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    process,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89188609-b53a-46c3-95a2-f2709606d979",
   "metadata": {
    "id": "89188609-b53a-46c3-95a2-f2709606d979"
   },
   "outputs": [],
   "source": [
    "from trl import CPOConfig, CPOTrainer\n",
    "\n",
    "args = CPOConfig(\n",
    "    logging_dir=\"logs\",\n",
    "    output_dir=\"ckpt\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=100,\n",
    "\n",
    "    max_length=512,\n",
    "    max_prompt_length=512,\n",
    "    dataset_num_proc=2,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    beta=0.1,\n",
    ")\n",
    "\n",
    "trainer = CPOTrainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836e9af-12e9-4342-8e4e-7098aed65772",
   "metadata": {
    "id": "8836e9af-12e9-4342-8e4e-7098aed65772",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.10 ORPO: Odds Ratio Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeea492-6448-461d-b1a3-7f868afb5a97",
   "metadata": {
    "id": "3aeea492-6448-461d-b1a3-7f868afb5a97"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e891864-89a1-4b27-afee-5b3b43e86a2c",
   "metadata": {
    "id": "2e891864-89a1-4b27-afee-5b3b43e86a2c",
    "outputId": "ad83067a-2ebe-4a4b-dff5-eeb3ccf103a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 160800\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"trl-internal-testing/hh-rlhf-trl-style\",\n",
    "    split=\"train\",\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777ed63-e2d4-4af5-a2d4-b3c5b6c80753",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "bd7fa5d9644143fda1717e605bd3bb25"
     ]
    },
    "id": "7777ed63-e2d4-4af5-a2d4-b3c5b6c80753",
    "outputId": "d965b3e8-9918-430c-b117-cc3dbc477414"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7fa5d9644143fda1717e605bd3bb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = (\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{{message['role'] + ': ' + message['content'] + '\\n\\n'}}\"\n",
    "        \"{% endfor %}{{ eos_token }}\"\n",
    "    )\n",
    "\n",
    "def process(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(\n",
    "        row[\"chosen\"], tokenize=False\n",
    "    )\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(\n",
    "        row[\"rejected\"], tokenize=False\n",
    "    )\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    process,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c349dc-cd07-49be-8dde-863f197f5a1d",
   "metadata": {
    "id": "d8c349dc-cd07-49be-8dde-863f197f5a1d"
   },
   "outputs": [],
   "source": [
    "from trl import ORPOConfig, ORPOTrainer\n",
    "\n",
    "args = ORPOConfig(\n",
    "    logging_dir=\"logs\",\n",
    "    output_dir=\"ckpt\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=100,\n",
    "\n",
    "    max_length=512,\n",
    "    max_prompt_length=512,\n",
    "    dataset_num_proc=2,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    beta=0.1,\n",
    ")\n",
    "\n",
    "trainer = ORPOTrainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
