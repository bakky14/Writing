{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5496dae6-0c2d-46c3-aee5-af396d044b7b",
      "metadata": {
        "id": "5496dae6-0c2d-46c3-aee5-af396d044b7b"
      },
      "source": [
        "# 8.2 Reward Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b5cfa2f-30ae-4412-8291-517d8450bf35",
      "metadata": {
        "id": "8b5cfa2f-30ae-4412-8291-517d8450bf35"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "15a35476-651a-40f6-869c-e8151aa98afa",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "15a35476-651a-40f6-869c-e8151aa98afa"
      },
      "source": [
        "# 8.3 SFT: Supervised Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3bc9d7a-530c-4ebf-bb19-3729bb240181",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "f3bc9d7a-530c-4ebf-bb19-3729bb240181"
      },
      "source": [
        "## quick start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12a2fd8-d077-477d-8a90-5cf862d5238a",
      "metadata": {
        "id": "b12a2fd8-d077-477d-8a90-5cf862d5238a",
        "outputId": "601da42e-85a2-4413-edc2-e94a1081d90d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/envs/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f81d091-1228-4c3a-b54b-fb72ee7d16ad",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "2f81d091-1228-4c3a-b54b-fb72ee7d16ad"
      },
      "source": [
        "## setup chat format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9044113-476e-46d5-908c-fafbe45ba17d",
      "metadata": {
        "id": "a9044113-476e-46d5-908c-fafbe45ba17d",
        "outputId": "9e02c07f-4641-43bf-dd0f-f5e3fb46c6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before: None\n",
            "after: {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import setup_chat_format\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "print(\"before:\", tokenizer.chat_template)\n",
        "\n",
        "# Set up the chat format with default 'chatml' format\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "print(\"after:\", tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e318f77a-aae9-4f95-8f5a-1ba9b764f7ae",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "e318f77a-aae9-4f95-8f5a-1ba9b764f7ae"
      },
      "source": [
        "## formatting func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd120275-41b6-4a03-a11a-999b03a4fcb2",
      "metadata": {
        "id": "bd120275-41b6-4a03-a11a-999b03a4fcb2"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['question'])):\n",
        "        text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=formatting_prompts_func,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1423f90d-e10a-4d3d-af75-1b6e233ecf81",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "1423f90d-e10a-4d3d-af75-1b6e233ecf81"
      },
      "source": [
        "## packing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8975e9d9-7f94-4913-8992-29b13271fec7",
      "metadata": {
        "id": "8975e9d9-7f94-4913-8992-29b13271fec7",
        "outputId": "7644f71b-8b13-453b-b64f-5ffaf4b58342",
        "colab": {
          "referenced_widgets": [
            "47733094048044c5bb4d79e4854c4510"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47733094048044c5bb4d79e4854c4510",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=True\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f4890b-2c56-44aa-bc4a-fbdc934491bb",
      "metadata": {
        "id": "28f4890b-2c56-44aa-bc4a-fbdc934491bb"
      },
      "source": [
        "## model_init_kwargs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5524b214-0dfc-4813-85d8-dd9752918da7",
      "metadata": {
        "id": "5524b214-0dfc-4813-85d8-dd9752918da7",
        "outputId": "d32456b3-570e-47f9-df64-594c33ca38d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    model_init_kwargs={\n",
        "        \"torch_dtype\": torch.bfloat16,\n",
        "        \"attn_implementation\": \"flash_attention_2\"\n",
        "    },\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3ed1d7-d403-443f-9cba-f70cd22e586c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "6a3ed1d7-d403-443f-9cba-f70cd22e586c"
      },
      "source": [
        "## peft_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b95c9ee-80f8-4d63-a25a-0ea470e4d92c",
      "metadata": {
        "id": "6b95c9ee-80f8-4d63-a25a-0ea470e4d92c",
        "outputId": "5f9705b6-1524-4170-d3c4-709b4383795f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/envs/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    peft_config=peft_config\n",
        ")\n",
        "\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e6db38-3ef1-4582-8ace-554e93842d2b",
      "metadata": {
        "id": "86e6db38-3ef1-4582-8ace-554e93842d2b",
        "outputId": "cec230c3-47e9-40af-a697-a521a5df4a98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/envs/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    model_init_kwargs={\n",
        "        \"torch_dtype\": torch.bfloat16,\n",
        "        \"load_in_4bit\": True,\n",
        "        \"attn_implementation\": \"flash_attention_2\"\n",
        "    },\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d77337f9-36d8-4992-83e5-9033466ac11d",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "d77337f9-36d8-4992-83e5-9033466ac11d"
      },
      "source": [
        "## model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d214967f-c72f-4acd-af1d-10c86a5a955d",
      "metadata": {
        "id": "d214967f-c72f-4acd-af1d-10c86a5a955d",
        "outputId": "15fbe37a-ca89-4c61-82ca-4145fe23fcdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModelConfig(model_name_or_path='facebook/opt-350m', model_revision='main', torch_dtype=None, trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from trl import ModelConfig, SFTTrainer, get_kbit_device_map, get_peft_config, get_quantization_config\n",
        "\n",
        "model_config = ModelConfig(\n",
        "    model_name_or_path=\"facebook/opt-350m\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    load_in_4bit=True,\n",
        "    use_peft=True,\n",
        ")\n",
        "model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a68533b-4302-4c19-a1a1-53f6f32433a2",
      "metadata": {
        "id": "9a68533b-4302-4c19-a1a1-53f6f32433a2",
        "outputId": "c6c8a5d2-2c4b-417d-8104-f4b6f06c9561"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BitsAndBytesConfig {\n",
              "  \"_load_in_4bit\": true,\n",
              "  \"_load_in_8bit\": false,\n",
              "  \"bnb_4bit_compute_dtype\": \"float32\",\n",
              "  \"bnb_4bit_quant_type\": \"nf4\",\n",
              "  \"bnb_4bit_use_double_quant\": false,\n",
              "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
              "  \"llm_int8_has_fp16_weight\": false,\n",
              "  \"llm_int8_skip_modules\": null,\n",
              "  \"llm_int8_threshold\": 6.0,\n",
              "  \"load_in_4bit\": true,\n",
              "  \"load_in_8bit\": false,\n",
              "  \"quant_method\": \"bitsandbytes\"\n",
              "}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "quantization_config = get_quantization_config(model_config)\n",
        "quantization_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9b17da-2c01-451e-b010-2322ea621a43",
      "metadata": {
        "id": "dc9b17da-2c01-451e-b010-2322ea621a43",
        "outputId": "3c2aa347-6cba-4bb4-d7e7-814e1fc773c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'': 0}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_kbit_device_map()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05dedd54-bf54-4f29-8dc5-11b6fdcce821",
      "metadata": {
        "id": "05dedd54-bf54-4f29-8dc5-11b6fdcce821",
        "outputId": "504b3118-75f2-4cdd-ac02-202d8820ef65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_config = get_peft_config(model_config)\n",
        "peft_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa248ced-71b6-4161-ad11-b8faa315342c",
      "metadata": {
        "id": "aa248ced-71b6-4161-ad11-b8faa315342c"
      },
      "outputs": [],
      "source": [
        "torch_dtype = (\n",
        "    model_config.torch_dtype\n",
        "    if model_config.torch_dtype in [\"auto\", None]\n",
        "    else getattr(torch, model_config.torch_dtype)\n",
        ")\n",
        "model_kwargs = dict(\n",
        "    revision=model_config.model_revision,\n",
        "    trust_remote_code=model_config.trust_remote_code,\n",
        "    attn_implementation=model_config.attn_implementation,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model_config.model_name_or_path,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    model_init_kwargs=model_kwargs,\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a065aa5-29bc-460d-a4b3-a7b40554e926",
      "metadata": {
        "id": "9a065aa5-29bc-460d-a4b3-a7b40554e926"
      },
      "source": [
        "## neftune_noise_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850a00bf-d498-4aff-8875-920855f26f32",
      "metadata": {
        "id": "850a00bf-d498-4aff-8875-920855f26f32",
        "outputId": "42e7df55-4fff-4594-8fff-7159f04bafd4",
        "colab": {
          "referenced_widgets": [
            "4e7d935b657a41df94ef7a3e517be0ec"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/envs/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e7d935b657a41df94ef7a3e517be0ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    \"facebook/opt-350m\",\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    neftune_noise_alpha=5,\n",
        ")\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca0099b-f7b3-4adf-acdd-2965272152e7",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "4ca0099b-f7b3-4adf-acdd-2965272152e7"
      },
      "source": [
        "# 8.4 PPO: Proximal Policy Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7d1cfb-7f23-4c2a-9076-cf418475e7ae",
      "metadata": {
        "id": "af7d1cfb-7f23-4c2a-9076-cf418475e7ae",
        "outputId": "8fb86660-09ac-4154-ba46-b7c037b6e5ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'Explain the moon landing to a 6 year old in a few sentences.'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\", split=\"train\")\n",
        "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
        "dataset = dataset.remove_columns([\"meta\", \"completion\"])\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ba0e01-19c2-4d3f-b064-b8c83b8ea2f9",
      "metadata": {
        "id": "36ba0e01-19c2-4d3f-b064-b8c83b8ea2f9",
        "outputId": "114793bc-a392-41c3-bdc8-5f306da40e61",
        "colab": {
          "referenced_widgets": [
            "5064ad68c2634a13b4812e2e973ebf26",
            "500cc0d6a77b4ad98ada9292e5793ebe",
            "b633e72373d24b70adf0771c52bb4c91",
            "3dff41913b4f43bfb2b80a7518490279",
            "796c24ca666d46febf220ca96ae5c493",
            "fe0c6ce1d91e410c87aaa9123097a385",
            "949f52a75df84baea5d799181e660966",
            "133cf46696d64e578944cda00bdb348d"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5064ad68c2634a13b4812e2e973ebf26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "500cc0d6a77b4ad98ada9292e5793ebe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b633e72373d24b70adf0771c52bb4c91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3dff41913b4f43bfb2b80a7518490279",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "796c24ca666d46febf220ca96ae5c493",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe0c6ce1d91e410c87aaa9123097a385",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "949f52a75df84baea5d799181e660966",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "133cf46696d64e578944cda00bdb348d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=\"gpt2\",\n",
        "    learning_rate=1.41e-5,\n",
        ")\n",
        "model = (\n",
        "    AutoModelForCausalLMWithValueHead\n",
        "    .from_pretrained(config.model_name)\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "# TDDO: 이거 eos 붙여서 학습해야하나?\n",
        "# tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9ecbf0-a6a9-4c84-aad5-45fb43577d94",
      "metadata": {
        "id": "8e9ecbf0-a6a9-4c84-aad5-45fb43577d94",
        "outputId": "1fa1853e-b946-4db7-8887-9f7c2ddf3766",
        "colab": {
          "referenced_widgets": [
            "a8be8aad2526439681bc032be64ae6fe",
            "4708632c30f44a598738ce3aa46f37c3",
            "5d943d9a509c4037b86dc6e5b7f3f0cd",
            "b16c76bbdde44176b92f131dd012971a",
            "c00a1781f2c6403c99d55e1685ace586",
            "49bc2716b98b483289a192dfe0624965"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8be8aad2526439681bc032be64ae6fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4708632c30f44a598738ce3aa46f37c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d943d9a509c4037b86dc6e5b7f3f0cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b16c76bbdde44176b92f131dd012971a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c00a1781f2c6403c99d55e1685ace586",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49bc2716b98b483289a192dfe0624965",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "reward_model = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"lvwerra/distilbert-imdb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a32441-343d-459b-aece-3d86226ff132",
      "metadata": {
        "id": "d7a32441-343d-459b-aece-3d86226ff132",
        "outputId": "775ec2ff-81b3-46e1-fcf5-57977c73825e",
        "colab": {
          "referenced_widgets": [
            "d9b50e3906784aa08f998aa03b7df5b8"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9b50e3906784aa08f998aa03b7df5b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize(sample):\n",
        "    # eos 붙여서?\n",
        "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
        "    return sample\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd33d74-77e8-43b2-86b6-bbcd28775ac7",
      "metadata": {
        "id": "2bd33d74-77e8-43b2-86b6-bbcd28775ac7"
      },
      "outputs": [],
      "source": [
        "from trl import PPOTrainer\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    model=model,\n",
        "    config=config,\n",
        "    dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3bbd0c-406d-4e16-9cfe-b32218a7dd4c",
      "metadata": {
        "id": "fd3bbd0c-406d-4e16-9cfe-b32218a7dd4c"
      },
      "outputs": [],
      "source": [
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840b488b-4a31-4ea6-b4a9-266ba04f3593",
      "metadata": {
        "id": "840b488b-4a31-4ea6-b4a9-266ba04f3593"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
        "    for batch in tqdm(ppo_trainer.dataloader):\n",
        "        query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "        # Rollout: 학습할 모델로 문장 생성\n",
        "        response_tensors = ppo_trainer.generate(\n",
        "            query_tensors,\n",
        "            **generation_kwargs\n",
        "        )\n",
        "        batch[\"response\"] = [\n",
        "            tokenizer.decode(r.squeeze())\n",
        "            for r in response_tensors\n",
        "        ]\n",
        "\n",
        "        # Evaluate: Reward 모델로 점수 부여\n",
        "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "        pipe_outputs = reward_model(texts)\n",
        "        rewards = [\n",
        "            torch.tensor(output[1][\"score\"])\n",
        "            for output in pipe_outputs\n",
        "        ]\n",
        "\n",
        "        # Optimization: ppo 학습 진행\n",
        "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "        ppo_trainer.log_stats(stats, batch, rewards)\n",
        "\n",
        "# 모델 저장\n",
        "ppo_trainer.save_pretrained(\"my_ppo_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac04b52-2d2c-49d7-83ad-909964cc3678",
      "metadata": {
        "id": "bac04b52-2d2c-49d7-83ad-909964cc3678"
      },
      "outputs": [],
      "source": [
        "ppo_trainer.tr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd4f05fe-ffa6-4c8f-bf38-787ad4643659",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "cd4f05fe-ffa6-4c8f-bf38-787ad4643659"
      },
      "source": [
        "# 8.5 Best of N Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a53734-522b-4f73-a684-de930a0d5591",
      "metadata": {
        "id": "87a53734-522b-4f73-a684-de930a0d5591",
        "outputId": "de38ba6a-8300-4c05-9f38-f164bdd5caff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, GenerationConfig\n",
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "from trl.core import LengthSampler\n",
        "from trl.extras import BestOfNSampler\n",
        "\n",
        "ref_model_name = \"gpt2\"\n",
        "reward_model_name = \"gpt2\"\n",
        "device = torch.device(\"cuda\")\n",
        "ref_model = (\n",
        "    AutoModelForCausalLMWithValueHead\n",
        "    .from_pretrained(ref_model_name)\n",
        "    .to(device)\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(ref_model_name)\n",
        "\n",
        "reward_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=reward_model_name,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def queries_to_scores(list_of_strings):\n",
        "    return [output[\"score\"] for output in reward_pipe(list_of_strings)]\n",
        "\n",
        "best_of_n = BestOfNSampler(\n",
        "    ref_model,\n",
        "    tokenizer,\n",
        "    queries_to_scores,\n",
        "    length_sampler=LengthSampler(10, 128),\n",
        "    sample_size=5,\n",
        "    n_candidates=2,\n",
        "    generation_config=GenerationConfig(\n",
        "        min_length= -1,\n",
        "        top_k=0.0,\n",
        "        top_p= 1.0,\n",
        "        do_sample= True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b73ae3c-328e-4259-a6b0-f1d8046f07c0",
      "metadata": {
        "id": "5b73ae3c-328e-4259-a6b0-f1d8046f07c0",
        "outputId": "0d60abae-1329-4c98-c526-4079db459b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what is love? Love? Where can I find it?\n",
            "\n",
            "Love love, what are you love loving?\n",
            "\n",
            "Love love, what is the Holiness to you?\n",
            "\n",
            "Love feeling love, what is the Glory of God in you?\n",
            "\n",
            "Love love, what does it mean to love?\n",
            "\n",
            "Love. Love who has\n",
            "================================================== \n",
            "\n",
            "what is love? How has love been changed? How things would change under God? Do God's desires continue with the child? Does love diminish with society's powers? Should people control themselves? At times these questions have seemed theological, yet of all the demanding questions that Jesus confronts, surely we get the most devastated reading.\n",
            "\n",
            "John Piper's\n",
            "================================================== \n",
            "\n"
          ]
        }
      ],
      "source": [
        "result = best_of_n.generate(\n",
        "    tokenizer(\"what is love?\", return_tensors=\"pt\").input_ids[0],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "for r in result[0]:\n",
        "    print(r)\n",
        "    print(\"=\" * 50, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30df7066-88e8-4b18-a4f1-aeb81affbab2",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "30df7066-88e8-4b18-a4f1-aeb81affbab2"
      },
      "source": [
        "# 8.6 DPO: Directi Preference Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "137b9de5-2ad5-49a5-bf76-44fc4e51250c",
      "metadata": {
        "id": "137b9de5-2ad5-49a5-bf76-44fc4e51250c",
        "outputId": "b0e90270-2547-4cf1-a91a-b4e1b0eb5e39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['chosen', 'rejected', 'prompt'],\n",
              "    num_rows: 160800\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"trl-internal-testing/hh-rlhf-trl-style\",\n",
        "    split=\"train\",\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bafbb629-0563-47ab-ae37-ebf6bf150285",
      "metadata": {
        "id": "bafbb629-0563-47ab-ae37-ebf6bf150285"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "ref_model_name = \"gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3135c81-ec72-4291-a642-643ee4c3b3e9",
      "metadata": {
        "id": "e3135c81-ec72-4291-a642-643ee4c3b3e9",
        "outputId": "60170dd5-e00d-4bec-bc66-3a5164a8ea6c",
        "colab": {
          "referenced_widgets": [
            "b341a81728f946d18f152675e369db0b"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b341a81728f946d18f152675e369db0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/160800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user: What kind of noises did dinosaurs make?\n",
            "\n",
            "assistant: Humans and dinosaurs didn’t live at the same time, so it’s really hard to say. The best place to find out what noises dinosaurs made would be\n",
            "\n",
            "user: yes they did\n",
            "\n",
            "assistant: to guess, and that would probably require lots of reading and a certain amount of imagination, so we’re not really prepared to do that.\n",
            "\n",
            "user: you cant read\n",
            "\n",
            "assistant: You can read?\n",
            "\n",
            "<|endoftext|>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        }
      ],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = \"{% for message in messages %}{{message['role'] + ': ' + message['content'] + '\\n\\n'}}{% endfor %}{{ eos_token }}\"\n",
        "\n",
        "def process(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    process,\n",
        "    num_proc=2,\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "print(dataset[1]['chosen'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce40d91c-11ac-42e7-b6d5-db6d30d3f47f",
      "metadata": {
        "id": "ce40d91c-11ac-42e7-b6d5-db6d30d3f47f"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    logging_dir=\"logs\",\n",
        "    output_dir=\"ckpt\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=100,\n",
        "\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "        model,\n",
        "        ref_model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=args,\n",
        "        train_dataset=dataset,\n",
        "        max_length=512,\n",
        "        max_prompt_length=512,\n",
        "        dataset_num_proc=2,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7c0149-1111-4225-aca4-a3e47efcf6c5",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8d7c0149-1111-4225-aca4-a3e47efcf6c5"
      },
      "source": [
        "# 8.7 KTO: Kahneman-Tversky Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667bfd19-05b1-4194-874d-0adfe4bc0bd6",
      "metadata": {
        "id": "667bfd19-05b1-4194-874d-0adfe4bc0bd6",
        "outputId": "8d912193-6661-4a99-da33-ff8cf61d79d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'completion', 'label'],\n",
              "    num_rows: 13500\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"trl-lib/kto-mix-14k\",\n",
        "    split=\"train\",\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c203c2-8312-4d0c-bfdd-e33a9a433db5",
      "metadata": {
        "id": "92c203c2-8312-4d0c-bfdd-e33a9a433db5",
        "outputId": "39c7a047-b578-4744-fe19-205940e516b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"trl-lib/qwen1.5-1.8b-sft\"\n",
        "ref_model_name = \"trl-lib/qwen1.5-1.8b-sft\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec5124d-3fee-48c0-bc8c-7d0c6ae1ca1b",
      "metadata": {
        "id": "dec5124d-3fee-48c0-bc8c-7d0c6ae1ca1b",
        "outputId": "0cf2875b-3ea7-4f84-cd4a-6b39c2ea140d",
        "colab": {
          "referenced_widgets": [
            "8e7e2da56d8a4c088312c649b7db7fbb"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e7e2da56d8a4c088312c649b7db7fbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/13500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>assistant\n",
            "No, the answer provided does not directly answer the question regarding the age of Julio Cesar Chavez when he fought Oscar De La Hoya. The provided information describes some general records held by Julio Cesar Chavez throughout his career. To answer your original question, let me provide the relevant information:\n",
            "\n",
            "Julio Cesar Chavez fought Oscar De La Hoya on June 7, 1996, in a match called \"Ultimate Glory.\" Chavez was born on July 12, 1962. To calculate his age at the time of the fight, we need to find the difference between the fight date and his birthdate.\n",
            "\n",
            "From July 12, 1962, to June 7, 1996, there are:\n",
            "- 33 years (from 1962 to 1995)\n",
            "- An additional year from his birthday in 1995 (July 12, 1995) to the fight date in 1996 (June 7, 1996), which is approximately 10 months and 26 days.\n",
            "\n",
            "Therefore, Julio Cesar Chavez was about 33 years and 10 months old when he fought Oscar De La Hoya.<|im_end|>\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        }
      ],
      "source": [
        "def process(row):\n",
        "    row[\"prompt\"] = tokenizer.apply_chat_template(row[\"prompt\"], tokenize=False)\n",
        "    row[\"completion\"] = tokenizer.apply_chat_template(row[\"completion\"], tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    process,\n",
        "    num_proc=2,\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "print(dataset[1]['completion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9db4d43-f542-48a8-811a-864cdd812bc9",
      "metadata": {
        "id": "f9db4d43-f542-48a8-811a-864cdd812bc9",
        "outputId": "f1739db4-0b7d-4669-93c0-2b77d92f1fa6",
        "colab": {
          "referenced_widgets": [
            "19d3b4cf9222423faf9440052ed1d71a",
            "669c3e39621849529e9fcc120aae6a68",
            "f1e796dc36b542a4a0ce1175340e979d",
            "cfc6f5710f8f4928830b8c962baaae6d",
            "5122383a6364497a9f8affdda801fd77",
            "27681098985c47fdaa44f201db492db0"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19d3b4cf9222423faf9440052ed1d71a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/13500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "669c3e39621849529e9fcc120aae6a68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting KL train dataset:   0%|          | 0/13500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1e796dc36b542a4a0ce1175340e979d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing tokenized train dataset (num_proc=2):   0%|          | 0/13500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfc6f5710f8f4928830b8c962baaae6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing tokenized train KL dataset (num_proc=2):   0%|          | 0/13500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5122383a6364497a9f8affdda801fd77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filtering desirable examples (num_proc=2):   0%|          | 0/13500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27681098985c47fdaa44f201db492db0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filtering undesirable examples (num_proc=2):   0%|          | 0/13500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.14 GiB (GPU 0; 11.99 GiB total capacity; 40.06 GiB already allocated; 0 bytes free; 40.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m KTOConfig(\n\u001b[1;32m      5\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     undesirable_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m KTOTrainer(\n\u001b[1;32m     24\u001b[0m     model,\n\u001b[1;32m     25\u001b[0m     ref_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/trl/trainer/kto_trainer.py:1092\u001b[0m, in \u001b[0;36mKTOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m compute_loss_context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_has_been_casted_to_bf16 \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compute_loss_context_manager():\n\u001b[0;32m-> 1092\u001b[0m     loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_loss_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# Make sure to move the loss to the device the original accumulating loss is at back in the `Trainer` class:\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/trl/trainer/kto_trainer.py:1019\u001b[0m, in \u001b[0;36mKTOTrainer.get_batch_loss_metrics\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m   1010\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1011\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: (v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1013\u001b[0m (\n\u001b[1;32m   1014\u001b[0m     policy_chosen_logps,\n\u001b[1;32m   1015\u001b[0m     policy_rejected_logps,\n\u001b[1;32m   1016\u001b[0m     policy_chosen_logits,\n\u001b[1;32m   1017\u001b[0m     policy_rejected_logits,\n\u001b[1;32m   1018\u001b[0m     policy_KL_logps,\n\u001b[0;32m-> 1019\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# if reference_logps in batch use them, otherwise use the reference model\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference_logps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/trl/trainer/kto_trainer.py:913\u001b[0m, in \u001b[0;36mKTOTrainer.forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    908\u001b[0m         KL_logits \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m    909\u001b[0m             batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL_completion_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    910\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL_completion_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    911\u001b[0m         )\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m--> 913\u001b[0m     completion_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompletion_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompletion_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    918\u001b[0m completion_logps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_logps(\n\u001b[1;32m    919\u001b[0m     completion_logits,\n\u001b[1;32m    920\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    923\u001b[0m     label_pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_pad_token_id,\n\u001b[1;32m    924\u001b[0m )\n\u001b[1;32m    926\u001b[0m KL_logps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_logps(\n\u001b[1;32m    927\u001b[0m     KL_logits,\n\u001b[1;32m    928\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL_completion_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m     label_pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_pad_token_id,\n\u001b[1;32m    932\u001b[0m )\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1186\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1173\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1174\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1175\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1183\u001b[0m )\n\u001b[1;32m   1185\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1186\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1189\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.14 GiB (GPU 0; 11.99 GiB total capacity; 40.06 GiB already allocated; 0 bytes free; 40.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "from trl import KTOTrainer, KTOConfig\n",
        "\n",
        "args = KTOConfig(\n",
        "    logging_dir=\"logs\",\n",
        "    output_dir=\"ckpt\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=100,\n",
        "\n",
        "    max_length=512,\n",
        "    max_prompt_length=512,\n",
        "    remove_unused_columns=False,\n",
        "    dataset_num_proc=2,\n",
        "\n",
        "    beta=0.1,\n",
        "    desirable_weight=1.0,\n",
        "    undesirable_weight=1.0,\n",
        ")\n",
        "\n",
        "trainer = KTOTrainer(\n",
        "    model,\n",
        "    ref_model,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029fcdfa-17ac-4092-a351-6efb74f56610",
      "metadata": {
        "id": "029fcdfa-17ac-4092-a351-6efb74f56610"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "15acec3a-8f45-4971-8fa1-23922df7d55a",
      "metadata": {
        "id": "15acec3a-8f45-4971-8fa1-23922df7d55a"
      },
      "source": [
        "# 8.8 CPO: Contrastive Preference Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778b9d0a-f47e-4f0a-a927-6ac63b9a667c",
      "metadata": {
        "id": "778b9d0a-f47e-4f0a-a927-6ac63b9a667c",
        "outputId": "bf9af8d4-9bb0-4112-effc-debaf5b452b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['chosen', 'rejected', 'prompt'],\n",
              "    num_rows: 160800\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"trl-internal-testing/hh-rlhf-trl-style\",\n",
        "    split=\"train\",\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bccba57-34fb-4c38-9391-ce039faad2d1",
      "metadata": {
        "id": "3bccba57-34fb-4c38-9391-ce039faad2d1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45127d14-aece-4131-8a5c-9a032ecc4e0a",
      "metadata": {
        "id": "45127d14-aece-4131-8a5c-9a032ecc4e0a",
        "outputId": "195e9900-fa83-4e5d-e789-e947f1ecdc36",
        "colab": {
          "referenced_widgets": [
            "4bffb2fabd194b20b26395fd156060e2"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bffb2fabd194b20b26395fd156060e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/160800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nsw/.pyenv/versions/3.10.13/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        }
      ],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = \"{% for message in messages %}{{message['role'] + ': ' + message['content'] + '\\n\\n'}}{% endfor %}{{ eos_token }}\"\n",
        "\n",
        "def process(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    process,\n",
        "    num_proc=2,\n",
        "    load_from_cache_file=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89188609-b53a-46c3-95a2-f2709606d979",
      "metadata": {
        "id": "89188609-b53a-46c3-95a2-f2709606d979"
      },
      "outputs": [],
      "source": [
        "from trl import CPOConfig, CPOTrainer\n",
        "\n",
        "args = CPOConfig(\n",
        "    logging_dir=\"logs\",\n",
        "    output_dir=\"ckpt\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=100,\n",
        "\n",
        "    max_length=512,\n",
        "    max_prompt_length=512,\n",
        "    dataset_num_proc=2,\n",
        "    remove_unused_columns=False,\n",
        "\n",
        "    beta=0.1,\n",
        ")\n",
        "\n",
        "trainer = CPOTrainer(\n",
        "    model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}