{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bakky14/Writing/blob/main/src/3.2%20Transformers%20%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%20%EA%B0%9C%EC%9A%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647Bcw6sQgpK",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# 3.2.1 Transformers 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wba2spjoJEjh"
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "     datasets==2.20.0 \\\n",
        "     transformers==4.41.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsJnOrgnG-PX"
      },
      "outputs": [],
      "source": [
        "!pip list | grep transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO2pSDiFQd9l"
      },
      "source": [
        "# 3.2.2 Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cgbxFGvfOAU"
      },
      "source": [
        "### Tokenizer 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZiH6cA_HlDB"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "model_name = \"klue/bert-base\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0DIOVfyEga7"
      },
      "outputs": [],
      "source": [
        "help(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv3OfgoLJvQM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(tokenizer.vocab_size)\n",
        "print(tokenizer.get_vocab())\n",
        "print(tokenizer.special_tokens_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtUi16-Ep1LQ"
      },
      "source": [
        "### 토큰화 작업"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUzzQ2VuDSYp"
      },
      "outputs": [],
      "source": [
        "sentence = \"안녕하세요. 이건 테스트입니다.\"\n",
        "\n",
        "# 토큰화 작업\n",
        "tokens1 = tokenizer.tokenize(sentence)\n",
        "print(tokens1)\n",
        "\n",
        "# 토큰을 입력 식별자로 변환\n",
        "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "print(ids1)\n",
        "\n",
        "ids2 = tokenizer(sentence)\n",
        "print(ids2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crXmK6FBGK-i"
      },
      "outputs": [],
      "source": [
        "# 디코딩\n",
        "decoded_string1 = tokenizer.decode(ids1)\n",
        "print(decoded_string1)\n",
        "\n",
        "decoded_string2 = tokenizer.decode(ids2[\"input_ids\"])\n",
        "print(decoded_string2)\n",
        "\n",
        "decoded_string3 = tokenizer.decode(ids2[\"input_ids\"], skip_special_tokens=True)\n",
        "print(decoded_string3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97byTNIRqUS4"
      },
      "source": [
        "### 데이터셋 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ikwTmUPq1MW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"klue\", \"ynat\")\n",
        "raw_train_dataset = dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9s55FLSqbCw"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "model_name = \"klue/bert-base\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenized_examples = tokenizer(\n",
        "    raw_train_dataset[\"title\"],\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8kHofqjrFrX"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(sample):\n",
        "    return tokenizer(sample[\"title\"])\n",
        "\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    remove_columns=[\"guid\", \"title\", \"url\", \"date\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COnFaDkVE-dT"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOf-KsxAQSbN"
      },
      "source": [
        "# 3.2.3 DataCollator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UH-hhNFr_3h"
      },
      "source": [
        "### DataCollator 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T_e4v5IL-Sy"
      },
      "outputs": [],
      "source": [
        "print(tokenized_datasets[\"train\"][0][\"input_ids\"])\n",
        "print(type(tokenized_datasets[\"train\"][0][\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoTJPZ_kNkSS"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "batch = [tokenized_datasets[\"train\"][i] for i in range(8)]\n",
        "print([len(sample[\"input_ids\"]) for sample in batch])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "batch = data_collator(batch)\n",
        "pprint({k: v.size() for k, v in batch.items()})"
      ],
      "metadata": {
        "id": "_Y2-n_UIMh2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWBTuuDxsdSk"
      },
      "source": [
        "# 3.2.4 Model\n",
        "https://huggingface.co/docs/transformers/index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X37kVKkO3Gy8"
      },
      "source": [
        "### Model 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc0UovVNL5S3"
      },
      "outputs": [],
      "source": [
        "!ls ~/.cache/huggingface/hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoEm_nnEssmz"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "model = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "model = BertModel.from_pretrained(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vyKYdJGs9wv"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/MyBertModel/\"\n",
        "tokenizer.save_pretrained(model_path)\n",
        "model.save_pretrained(model_path)\n",
        "\n",
        "!ls -l {model_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOOXBnFFNGJb"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertModel.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1kffN8ptPEE"
      },
      "source": [
        "### Model 추론 실습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md6C1eb6DyJN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "model = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "model = BertForMaskedLM.from_pretrained(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "   logits = model(**inputs).logits\n",
        "\n",
        "# retrieve index of [MASK]\n",
        "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]"
      ],
      "metadata": {
        "id": "pkXGQBWzQoLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "\n",
        "tokenizer.decode(predicted_token_id)"
      ],
      "metadata": {
        "id": "6_imbHrPRItQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ0pSh2Mvbxe"
      },
      "source": [
        "# 3.2.5 AutoClass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_vmPC0Ztqyi"
      },
      "source": [
        "### AutoClass로 Tokenizer, Model 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgRo70CE2tzT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "                        #-----------------------------------#\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "   logits = model(**inputs).logits\n",
        "\n",
        "# retrieve index of [MASK]\n",
        "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "tokenizer.decode(predicted_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnswkEKQuFDw"
      },
      "source": [
        "# 3.2.7 Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZi0vDCnuNzw"
      },
      "source": [
        "### 허깅페이스 허브에 있는 모델 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmRiL8iEuL9R"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(task=\"text-classification\",\n",
        "                model=\"google-bert/bert-base-uncased\")\n",
        "\n",
        "print(pipe(\"유튜브 내달 2일까지 크리에이터 지원 공간 운영\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq1mzMbyuVaS"
      },
      "source": [
        "### 미세조정 모델 경로로 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quWNnuH7vCsp",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"/content/MyBertModel/\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "CDDrIu4MXRTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(task=\"text-classification\",\n",
        "                tokenizer=tokenizer,\n",
        "                model=model)\n",
        "\n",
        "print(pipe(\"유튜브 내달 2일까지 크리에이터 지원 공간 운영\"))"
      ],
      "metadata": {
        "id": "ZAP0t1VBXCKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FPuHy65vCsq"
      },
      "outputs": [],
      "source": [
        "model_name = \"google-bert/bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = pipeline(task=\"text-classification\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer)\n",
        "\n",
        "print(pipeline(\"유튜브 내달 2일까지 크리에이터 지원 공간 운영\"))"
      ],
      "metadata": {
        "id": "9I0_qtpTXbWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BEWh3UT1IAx"
      },
      "source": [
        "### 직접 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktuESQQpvCsq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "model_name = \"google-bert/bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.cuda().eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(\n",
        "        **tokenizer(\n",
        "            \"유튜브 내달 2일까지 크리에이터 지원 공간 운영\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "    )\n",
        "    result = torch.softmax(output.logits.cpu(), -1)\n",
        "\n",
        "result = [\n",
        "    {\"label\": f\"LABEL_{l}\", \"score\": result[i, l].item()}\n",
        "    for i, l in enumerate(result.argmax(-1))\n",
        "]\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgfBKX8uKdWR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}